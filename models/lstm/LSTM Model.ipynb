{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to article explainig code:\n",
    "<url>https://cnvrg.io/pytorch-lstm/</url>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read processed data\n",
    "df = pd.read_csv('../../dataframes/monthly_processed.csv', index_col='date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable definitions\n",
    "num_epochs = 1                  # number of epochs\n",
    "learning_rate = 0.001           # 0.001 lr\n",
    "\n",
    "seq_length = 36                 # sequence length of sequence given to the LSTM layer\n",
    "pred_length = 12                # prediction length of output sequence produced by LSTM layer \n",
    "                                # (multiples of twelve since there is yearly seasonality)\n",
    "\n",
    "input_size = len(df.columns)    # number of features (amount of input variables)\n",
    "output_size = 1                 # size of output (peru fish oil price so just one variable)\n",
    "hidden_size = 5                 # number of features in hidden state (this number should be treated as a hyperparameter)\n",
    "num_layers = 1                  # number of stacked lstm layers\n",
    "\n",
    "num_classes = pred_length       # number of output classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create list of sequences of length seq_length\n",
    "def create_series(data):\n",
    "    # the amount of time-steps for which we have data\n",
    "    total_timesteps = len(data)\n",
    "\n",
    "    # arrays that hold the input sequences and the corresponding target sequences respectively.\n",
    "    sequencesX, sequencesY = [], []\n",
    "    if (total_timesteps < seq_length + pred_length):\n",
    "        # verify that enough time-steps of the data are available to create input sequence with their corresponding targets.\n",
    "        raise Exception(\"This dataframe cannot be used to create sequences of length \" + str(seq_length + pred_length))\n",
    "    \n",
    "    # create all possible sequences of length seq_length and their \n",
    "    for i in range(total_timesteps - seq_length - pred_length):\n",
    "        # append the input sequence\n",
    "        sequencesX.append(data.iloc[i:(i+seq_length), :])\n",
    "        # append the corresponding (target) output sequence\n",
    "        sequencesY.append(data.iloc[(i+seq_length):(i+seq_length+pred_length), 0:1])\n",
    "    \n",
    "    # return the sequences\n",
    "    return np.array(sequencesX), np.array(sequencesY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block the X and y sets are created.\n",
    "\n",
    "The shape of `X` is as follows:<br>\n",
    "The element `X[s][i][j]` contains the value of the $j$'th input variable (out of the `len(df.columns)` different input variables) at the $i$'th time-step (out of the `seq_length` time-steps in the sequence) for the $s$'th sequence in the data set `X`.<br>\n",
    "\n",
    "The shape of `y` is as follows:<br>\n",
    "The element `y[s][i][0]` contains the value of the peru fish oil price (target) at the $i$'th time-step (out of the `pred_length` time-steps in the sequence) for the $s$'th sequence in the data set `y`.<br>\n",
    "The last zero-index is used because all the values are wrapped in length one arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input/output shapes respectively: (108, 36, 7) (108, 12, 1)\n",
      "Testing input/output shapes respectively: (102, 36, 7) (102, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "# TODO: May also need a validation set to determine optimal hyperparameters\n",
    "df_train = df[(df.index > \"1996-12-01\") & (df.index < \"2010-01-01\")]    # use only past 1997 due to geopolitical changes\n",
    "df_test = df[df.index >= \"2010-01-01\"]\n",
    "\n",
    "# create the sequences to train on\n",
    "X_train, y_train = create_series(df_train)\n",
    "\n",
    "# create the sequences to test on\n",
    "X_test, y_test = create_series(df_test)\n",
    "\n",
    "# print shapes\n",
    "print(\"Training input/output shapes respectively:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing input/output shapes respectively:\", X_test.shape, y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are converted to PyTorch tensors and the shapes are printed.<br>\n",
    "The input shape is of the form `(nr_of_sequences, seq_length, input_size)`.<br>\n",
    "The output shape is of the form `(nr_of_sequences, pred_length, output_size)`.\n",
    "\n",
    "If this needs to be reshaped the `torch.reshape(data, shape)` function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input/output shapes respectively: torch.Size([108, 36, 7]) torch.Size([108, 36, 7])\n",
      "Testing input/output shapes respectively: torch.Size([102, 36, 7]) torch.Size([102, 12, 1])\n"
     ]
    }
   ],
   "source": [
    "# convert data to pytorch variables\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test)) \n",
    "\n",
    "# print shapes\n",
    "print(\"Training input/output shapes respectively:\", X_train_tensors.shape, X_train_tensors.shape)\n",
    "print(\"Testing input/output shapes respectively:\", X_test_tensors.shape, y_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]),torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input size of the first fully connected layer is equal to `(hidden_size * seq_length)` because the LSTM outputs the hidden state (containing `hidden_size` variables) for every time-step in the input which contains `seq_length` time-steps. The output size of this layer is a hyperparameter for which a default of twice the amount of inputs is used.\n",
    "\n",
    "The lstm layer can also use an initial hidden state and initial cell state for each time-step in the sequence of `seq_length` but since these are not provided it defaults to zero.<br>\n",
    "See the input section of the <a url=https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html>documentation</a> for more information.\n",
    "\n",
    "The output of the lstm layer is `output, (output_hidden_states, output_cell_states)` where `output` contains for every sequence in the provided `input` the hidden state at each time-step.<br>\n",
    "Furthermore, `output_hidden_states` and `output_cell_states` contain for every sequence the hidden state and cell state of the last time-step, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes      # number of classes\n",
    "        self.num_layers = num_layers        # number of layers\n",
    "        self.input_size = input_size        # input size\n",
    "        self.hidden_size = hidden_size      # hidden state\n",
    "\n",
    "        fc_1_input_size = hidden_size * seq_length\n",
    "        fc_1_output_size = hidden_size * seq_length * 2     # output size of first fully connected layer (hyperparameter)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) # lstm\n",
    "        self.fc_1 = nn.Linear(fc_1_input_size, fc_1_output_size) # first fully connected layer\n",
    "        self.fc_2 = nn.Linear(fc_1_output_size, num_classes) # second fully connected layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # propagate input through LSTM\n",
    "        output, (output_hidden_states, output_cell_states) = self.lstm(input) # retrieve output from lstm by providing input (the initial hidden and cell states can be set but default sets to zero)\n",
    "        # flattened_output_hidden_states = torch.flatten(output_hidden_states) # flattened output hidden states\n",
    "        # flattened_output_cell_states = torch.flatten(output_cell_states) # flattened output cell states\n",
    "        # flattened_output = torch.cat((flattened_output_hidden_states, flattened_output_cell_states), 0) # concatenate the flattened output hidden and cell states (among dimension 0 since they only have one)\n",
    "        flattened_output = torch.flatten(output) # flattened output\n",
    "        out = self.relu(flattened_output) # relu\n",
    "        out = self.fc_1(out) # first fully connected layer\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc_2(out) # second fully connected layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the lstm layer \n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers) # lstm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function and weight updating method\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.forward(X_train_tensors[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seppe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([12, 1])) that is different to the input size (torch.Size([12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.05533\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "  # using batches of size 1\n",
    "  for sequence in range(X_train_tensors.size(0)):\n",
    "    outputs = lstm.forward(X_train_tensors[sequence]) # forward pass\n",
    "    optimizer.zero_grad() # calculate the gradient, manually setting to 0\n",
    "  \n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, y_train_tensors[sequence])\n",
    "  \n",
    "    loss.backward() # calculates the loss of the loss function\n",
    "  \n",
    "    optimizer.step() # improve from loss, i.e backprop\n",
    "  if epoch % 10 == 0:\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for the test set\n",
    "\n",
    "# compute loss on test set\n",
    "\n",
    "# plot test set predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fc28af60476edc3e27144daec4d860a5bfd0f3e7bdb17e7edfc0e13d60e755d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
